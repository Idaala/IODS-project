

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Exercise 4. Clustering and classification**

The data used in this exercise comprises the housing values in the suburbs of Boston. The Boston data frame has 506 rows and 14 variables. More information can be found: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html

```{r}
library(magrittr)
library(MASS)
library(ggplot2)
library(tidyr)
library(corrplot)
library(GGally)


data("Boston")
str(Boston)
summary(Boston)

Boston %>%
  purrr::keep(is.numeric) %>% 
  tidyr::gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()


#graphical overview
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```

From the summary we can see that the variables are numerical. The chas variable is a dummy variable. From the graph we can see that the variable distributions are skewed. However the variable rm seems to be normally distributed. The correlation matrix shows the associations between the variables. The bigger and more colorful the circle is, the stronger the correlation is between the variables. There appears to be a strong positive relationship between property taxes (tax) and radial highway accessibility (rad). The lower status of the population (istat) and median value of owner-occupied homes in $1000s (medv) have a strong negative association. Additionally, strong negative association is between proportion of owner-occupied units built prior to 1940 (age) and weighted mean of distances to five Boston employment centres (dis).

## Standardizing the dataset

The Boston data contains only numerical values


```{r}
boston_scaled <- as.data.frame(scale(Boston))
summary(boston_scaled)
```


The scaled dataset's summary shows that after standardization, all variables fit into a normal distribution, with the mean of each variable being zero. We can create a categorical variable from a continuous one. We can create a categorical variable of the crime rate in the Boston dataset. 


```{r}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

## Dividing the dataset to train and test sets


```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows

ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

Linear Discriminant analysis is a classification (and dimension reduction) method. It finds the (linear) combination of the variables that separate the target variable classes. The target can be binary or multiclass variable. crime rate as the target variable and all the other variables in the dataset as predictor variables

```{r}
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit
```
As we can see LD1 captures 95% of differences. Between the groups, LD2 adds 3% to that, and LD3 adds about 1%.LDA can be visualized with a biplot.

```{r}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

Longer arrows represent more discrimination. However from the plot we can see that  target variable crime is well separated.

## Prediction

We split our data earlier so that we have the test set and the correct class labels. See how the LDA model performs when predicting on new (test) data.

```{r}
#The crime variable has already been saved as vector.
#It's also not required to remove the crime variable from the test data. 
## test <- dplyr::select(test, -crime)

lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)


```

The model accurately anticipated the highest crime rates. The total number of correctly predicted observations is the sum of the diagonal. Approximately 72.5 % of the projected values were placed in the same category as the correct values, based on a total of 102 observations. Only around 60% of the medium low and medium high grades were correct. When it came to low crime rate, there was more misunderstanding than when it came to high crime rates. 


## Reloading the Boston dataset and standardizing the dataset



```{r}
# Reload Boston dataset.
data("Boston")

# standardize variables
boston_scaled <- scale(Boston)

boston_scaled <- as.data.frame(boston_scaled)

dist_eu <- dist(boston_scaled)
summary(dist_eu)
```


## Clustering

The data will next be clustered using k-means. There are four classes in our LDA model that would be a good place to start. One way to decide the optimal number of clusters is to look how cluster sum of squares (WCSS) behaves when the number of cluster changes. The optimal number of clusters is when the total WCSS drops radically.

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

We can see that the WCSS drops radically in the case of 2 clusters. 


```{r}

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
#correlation
cor_matrixx<-cor(boston_scaled) %>% round(digits = 2)

# print the correlation matrix
cor_matrixx

# visualize the correlation matrix
corrplot(cor_matrixx, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)


```
The ideal number of clusters when utilizing euclidian distance appears to be two. We can see that rad and tax are correlated with Per capita crime rate by town (crime) (positive correlation). Also it seems like istat is somewhat correlated with crime rates. It is intuitive as people who have a lower status may commit more crimes. Because the rad and tax are so closely linked, it's difficult to tell what fosters crime in a certain area: strong connections or wealth as measured by taxes. It is a bit hard to evaluate these correlations. 